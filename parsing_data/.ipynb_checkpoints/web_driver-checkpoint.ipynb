{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dacb92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Analisys:\n",
    "    \n",
    "    def __init__(self, source: str, chapter: str|None, analisys_name: str, analisys_cost:str, analisys_comment: str,):              \n",
    "\n",
    "        self.source    = source\n",
    "        self.chapter     = chapter\n",
    "        self.analisys_name   = analisys_name\n",
    "        self.analisys_cost   = analisys_cost\n",
    "        self.analisys_comment  = analisys_comment\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"source = {self.source}, chapter = {self.chapter}, analisys_name = {self.analisys_name}, analisys_cost = {self.analisys_cost}, analisys_comment = {self.analisys_comment}\"\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return self.__dict__\n",
    "        \n",
    "    \n",
    "class WebDriver(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.urls: List[str] = []\n",
    "            \n",
    "    def _scan_page(self, url:str):\n",
    "        # Прописываем нестандартный заголовок, чтобы сайт не принял нас за бота, который парсит его данные :)\n",
    "        headers = requests.utils.default_headers()\n",
    "        headers.update({\n",
    "                    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                    \"user-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36',\n",
    "            })\n",
    "\n",
    "        response = requests.get(url=url, headers=headers)\n",
    "\n",
    "        if not response.ok:\n",
    "            raise ValueError(\"no content to parse\")\n",
    "\n",
    "        return BeautifulSoup(response.content.decode(), \"lxml\")            \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_analisys(self) :  \n",
    "        ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
